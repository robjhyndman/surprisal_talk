---
title: Anomaly detection using&nbsp;surprisals 
author: Rob J Hyndman
date: 28 October 2025
abstract: "I will discuss a probabilistic approach to anomaly detection based on extreme 'surprisal values' aka log scores, equal to minus the log density at each observation. The surprisal approach can be used for any collection of data objects, provided a probability density can be defined on the sample space. It can distinguish anomalies from legitimate observations in a heavy tail, and will identify anomalies that are undetected using methods based on distance measures. I will demonstrate the idea in various real data examples including univariate, multivariate and regression contexts, and when exploring more complicated data objects. I will also briefly outline the underlying theory when the density is known, and when it is estimated using a kernel density estimate. In the latter case, an innovative bandwidth selection method is used based on persistent homology."
toc: true
format:
  presentation-beamer:
    knitr:
      opts_chunk:
        dev: "CairoPDF"
    fig-width: 8
    fig-height: 4.25
    titlegraphic: bg-13.png
    titlecolor: white
include-in-header:
  text: |
    \usepackage{bm,bbm}
highlight-style: tango
execute:
  echo: false
  message: false
  warning: false
  cache: false
---

```{r}
#| label: load-packages
library(targets)
library(ggplot2)
```

# Anomalies and surprisals

## Old faithful eruptions

```{r}
tar_read(fig_old_faithful)
```

## Wine quality and prices

```{r}
tar_read(fig_wine)
```

## French mortality

```{r}
tar_read(fig_fr_mortality)
```

## French mortality

```{r}
tar_read(fig_fr_mortality2)
```

## Definitions of anomalies

\begin{block}{} 
\emph{an observation (or a subset of observations) which appears to be inconsistent with the remainder of that set of data.}\\\mbox{}\hfill (Barnett \& Lewis, 1978) 
\end{block}\pause

\begin{block}{}
\emph{an observation which deviates so much from other observations as to arouse suspicion it was generated by a different mechanism.}\hfill (Hawkins, 1980)
\end{block}\pause

## Definitions of anomalies
\fontsize{14}{16}\selectfont
\begin{alertblock}{Definition: Anomaly}
  Given a set of observations $\{y_{1},\dots,y_n\}$ drawn from probability distribution $F$, $y_i$ is an \textbf{anomaly} if 
  $$\int \mathbbm{1}(f(u) < f(y_i)) du < \alpha,$$
  where $\mathbbm{1}$ is the indicator function, $f$ is the generalized density of $F$, and $\alpha > 0$ is a chosen threshold.
\end{alertblock}\pause

* $y_i$ can be a scalar, vector or a more complex object
* $f$ can be a conditional density, and can be known or estimated

## Surprisals

\begin{alertblock}{Definition: Surprisal}
  The \textbf{surprisal} of an observation $y_i$ drawn from probability distribution $F$ with generalized density $f$ is defined as
  $$s_i = -\log f(y_i)$$
\end{alertblock}

* Better known as "log scores" in statistics.
* "Surprisal" coined by Tribus (1961).
* Average surprisal = entropy of random variable
* Sum of surprisals = negative log likelihood

## Anomaly detection using surprisals

Let $G(s) = P(S \le s)$ be the **surprisal distribution** where $S = -\log f(Y)$ and $Y \sim F$. 
$$G(s) = P(-\log f(Y) \le s) = P(f(Y) \ge e^{-s})$$
The **surprisal score** is
$$p_i = 1-G(s_i)$$
and an observation is an **anomaly** if $p_i < \alpha$.

## Anomaly detection using surprisals

e.g., $F\sim N(\mu,\sigma^2)$


# Extreme value theory and surprisals

## Extreme value theory

 * GEV
 * GPD

## Application to French mortality 

## Application to Wine quality and prices

# Lookout algorithm

## KDE

## Bandwidth selection

## Persistent homology


## Application to Wine quality and prices

# Conclusions

