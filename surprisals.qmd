---
title: Anomaly detection using&nbsp;surprisals 
author: Rob J Hyndman
date: 28 October 2025
abstract: "I will discuss a probabilistic approach to anomaly detection based on extreme 'surprisal values' aka log scores, equal to minus the log density at each observation. The surprisal approach can be used for any collection of data objects, provided a probability density can be defined on the sample space. It can distinguish anomalies from legitimate observations in a heavy tail, and will identify anomalies that are undetected using methods based on distance measures. I will demonstrate the idea in various real data examples including univariate, multivariate and regression contexts, and when exploring more complicated data objects. I will also briefly outline the underlying theory when the density is known, and when it is estimated using a kernel density estimate. In the latter case, an innovative bandwidth selection method is used based on persistent homology."
toc: true
format:
  presentation-beamer:
    knitr:
      opts_chunk:
        dev: "CairoPDF"
    fig-width: 8
    fig-height: 4.25
    titlegraphic: bg-13.png
    titlecolor: white
include-in-header:
  text: |
    \usepackage{bm,bbm}
    \def\E{\mathbb{E}}
    \def\Pr{\mathbb{P}}
highlight-style: tango
execute:
  echo: false
  message: false
  warning: false
  cache: false
---

```{r}
#| label: load-packages
library(targets)
library(ggplot2)
```

# Anomalies and surprisals

## Old faithful eruptions

```{r}
tar_read(fig_old_faithful)
```

## Wine quality and prices

```{r}
tar_read(fig_wine)
```

## French mortality

```{r}
tar_read(fig_fr_mortality)
```

## French mortality

```{r}
tar_read(fig_fr_mortality2)
```

## Definitions of anomalies

\begin{block}{} 
\emph{an observation (or a subset of observations) which appears to be inconsistent with the remainder of that set of data.}\\\mbox{}\hfill (Barnett \& Lewis, 1978) 
\end{block}\pause

\begin{block}{}
\emph{an observation which deviates so much from other observations as to arouse suspicion it was generated by a different mechanism.}\hfill (Hawkins, 1980)
\end{block}

## Definitions of anomalies
\fontsize{14}{16}\selectfont
\begin{alertblock}{Definition: Anomaly}
  Given a set of observations $\{y_{1},\dots,y_n\}$ drawn from probability distribution $F$, $y_i$ is an \textbf{anomaly} if 
  $$\Pr(f(Y) < f(y_i)) < \alpha$$
  where $Y\sim F$, $f$ is the generalized density of $F$, and $\alpha > 0$ is a chosen threshold.
\end{alertblock}\pause

* $y_i$ can be a scalar, vector or a more complex object
* $f$ can be a conditional density, and can be known or estimated

## Definitions of anomalies

```{r}
tar_read(fig61)
```

## Anomaly detection: Normal distribution
\vspace*{-0.2cm}

If $F\sim N(\mu,\sigma^2)$, then $p_i = 2\left[1 - \Phi(|y_i - \mu|/\sigma)\right]$\newline
Equivalent to a two-sided p-value from a z-score test.

```{r}
#| fig-height: 3.5
tar_read(fig_normal)
```

## Anomaly detection: Highest density regions
\vspace*{-0.2cm}

HDR with probability $1-\alpha$ is $R_\alpha = \{y: f(y) \ge c_\alpha\}$ where $c_\alpha$ is largest constant s.t. $\Pr(Y \in R_\alpha) \ge 1-\alpha$.\newline
An observation is an anomaly if $y_i \notin R_\alpha$.

```{r}
#| fig-height: 3.1
tar_read(fig_hdr)
```

## Surprisals

\begin{alertblock}{Definition: Surprisal}
  The \textbf{surprisal} of an observation $y_i$ drawn from probability distribution $F$ with generalized density $f$ is defined as
  $$s_i = -\log f(y_i)$$
\end{alertblock}

* Better known as "log scores" in statistics.
* "Surprisal" coined by Tribus (1961).
* Average surprisal = entropy of random variable
* Sum of surprisals = negative log likelihood

## Anomaly detection using surprisals

Let $G(s) = P(S \le s)$ be the **surprisal distribution** where $S = -\log f(Y)$ and $Y \sim F$. 
$$G(s) = P(-\log f(Y) \le s) = P(f(Y) \ge e^{-s})$$
The **surprisal score** is
$$p_i = 1-G(s_i)$$
and an observation is an **anomaly** if $p_i < \alpha$.

# Extreme value theory and surprisals

## Fisher-Tippett-Gnedenko theorem
\fontsize{13}{14}\selectfont
Consider $n$ iid rvs $S_1, \dots, S_n$ with cdf $G$ and $M_n = \max \{S_1, \dots, S_n\}$. If there exist sequences of constants $\{a_n>0\}$ and $\{b_n\}$ such that
$$P\left\{ (M_n - b_n)/a_n \leq z \right\} \rightarrow H(z) \quad \text{as} \quad n \to \infty,$$
for a non-degenerate cdf $H$, then 
$$H(z) = \exp\left\{ -\left[ 1 + \xi\left(\frac{z - \mu}{\sigma} \right)\right]^{-1/\xi} \right\}$$

* $\xi>0$: Fréchet distribution ($G$ heavy-tailed)
* $\xi\rightarrow0$: Gumbel distribution ($G$ light-tailed)
* $\xi<0$: Weibull distribution ($G$ bounded upper tail)

## Pickands-Balkema-De Haan theorem

If $G$ satisfies the FTG theorem, then the upper tail of $G$ can be approximated by the Generalized Pareto Distribution (GPD): 
$$\Pr(S \le u+s \mid S > u) = 1 - \left( 1 + \frac{\xi s}{\sigma_u} \right)^{-1/\xi}$$
for large enough $u$, where $\sigma_u = \sigma + \xi(u- \mu)$.

## Surprisals and EVT

* Suppose we have $n$ iid observations $Y_1,\dots,Y_n$ from distribution $F$ with density $f$.
* Let $S_i = -\log f(Y_i)$ be the surprisal of $Y_i$.
* Then $S_1,\dots,S_n$ are iid from the surprisal distribution $G(s) = P(S \le s)$.
* If $G$ satisfies the FTG theorem, then we can approximate the upper tail of $G$ by a GPD.

## Three-type theorem for surprisals

**A1: Sub-Gaussian**: $S =-\log f(Y)$ satisfies, for all $\lambda\in\mathbb{R}$, and some $\nu>0$,
$\E \exp\{\lambda (S-\E[S])\}\le \exp\{\lambda^2 \nu^2/2\}$.`\\`{=latex}
**A2: Sub-exponential**: $S$ is sub-exponential with parameters $\nu$ and $b$, i.e.,
$\E \exp\{\lambda(S-\E[S])\} \leq \exp\{\lambda^2\nu^2/ 2\}$ for all $|\lambda|< 1/b$.`\\`{=latex}
**A3: Polynomial**: $|S|$ has polynomial moments of order $\alpha\ge 1$; i.e., $\mathbb{E}[|S|^p] \le C^p$ for some $C>0$ such that $C^p-1>0$.

* A1 satisfied when $f$ has bounded support
* A2 satisfied when $f$ supported on $\mathbb{R}^d$ and log $f$ unbounded below (e.g., Gaussian)
* A3 satisfied when $f$ has polynomial tails (e.g., Student-t)

## Three-type theorem for surprisals
\fontsize{13}{14}\selectfont
Let $y_1,\dots,y_n$ be an iid sequence from $F$, $S =-\log f(Y)$ where $Y\sim F$, $s_i = -\log f(y_i)$, and $M_n=\max\{s_1,\dots,s_n\}$.

1. Under sub-Gaussian tails (A1):
$$
  \sup_{s:s>0}\left|\Pr\left\{\big|M_n-\E[S]\big|\ge \sqrt{2\nu^2 s}+\sqrt{2\nu^2\log(2n)}\right\}-e^{-s}\right|=o(1).
$$
2. Under sub-exponential tails (A2):
$$
  \sup_{s:s>1/b}\left|\Pr\Big\{\big|M_n-\E[S]\big|\ge (2b)s+(2b)\log(2n)\Big\}-e^{-e^{-s}}\right|=o(1).
$$
3. Under polynomial tails (A3):
$$
	\sup_{s:s>C}\left|\Pr\Big\{\big|M_n-\E[S]\big|\ge (Csn^{1/p})\Big\}-e^{-s^{-p}}\right|=o(1).
$$


## Three-type theorem for surprisals

* If surprisal has Gaussian like tails, then maximum surprisal is a reversed Weibull;
* If surprisal only has an exponential moment, then maximum surprisal is Gumbel; 
* If surprisal only has a polynomial moment, then maximum surprisal is Fréchet. 
 
**Corollary:** upper tail of the surprisal distribution can be approximated by a GPD, even if the assumed distribution $F$ is incorrect, provided one of A1--A3 is satisfied.

## Application to wine quality and prices

```{r}
tar_read(fig_wine)
```

## Application to wine quality and prices
\vspace*{-0.3cm}
Proposed model: $\log \text{Price} \mid \text{Points} \sim N(a + b\text{Points}, \sigma^2)$.

```{r}
#| fig-height: 3.8
tar_read(wine_model_plot)
```

## Application to wine quality and prices
\vspace*{-0.3cm}
Proposed model: $\log \text{Price} \mid \text{Points} \sim N(a + b\text{Points}, \sigma^2)$.

```{r}
#| fig-height: 3.8
tar_read(wine_loo_surprisals)
```

## Application to wine quality and prices
\vspace*{-0.3cm}
Proposed model: $\log \text{Price} \mid \text{Points} \sim N(a + b\text{Points}, \sigma^2)$.

```{r}
#| fig-height: 3.8
tar_read(wine_loo_anomalies)
```

\begin{textblock}{4.9}(1.9,2.0)
\begin{block}{}\footnotesize
GPD fitted to top 10\% of surprisals computed from LOO predictive densities.
\end{block}
\end{textblock}


\begin{textblock}{1.85}(13.9,0.95)
\begin{block}{}\small
\textcolor{red}{$\alpha = 0.001$}
\end{block}
\end{textblock}

## Application to wine quality and prices

```{r}
tar_read(fig_wine2)
```

\begin{textblock}{1.85}(13.9,0.95)
\begin{block}{}\small
\textcolor{red}{$\alpha = 0.001$}
\end{block}
\end{textblock}


## Application to wine quality and prices
\vspace*{-0.2cm}
\structure{Anomalies detected ($\alpha=0.001$):}
\fontsize{12}{14}\selectfont\vspace*{-0.3cm}

```{r}
tar_read(wine_anomalies) |>
  kableExtra::kbl(booktabs = TRUE, linesep="")
```

## French mortality

```{r}
tar_read(fig_fr_mortality)
```

## French mortality

```{r}
tar_read(fig_fr_mortality2)
```

## Application to French mortality 
\fontsize{14}{16}\selectfont

Model: $\log y_t \sim N(m_t, a_t)$ where $m_t$ and $a_t$ are locally and robustly estimated in a window of size $2h+1$ around time $t$:
\begin{align*}
\hat{m}_t &= \text{median}(\log y_{t-h},\dots,\log y_{t+h}) \\
\hat{a}_t &= 1.4826 \times \text{median}(|\log y_{t-h}-\hat{m}_t|,\dots,|\log y_{t+h}-\hat{m}_t|)
\end{align*}\pause\vspace*{-0.8cm}

* Male and female data from 1816--1999, over ages 0--85: `r NROW(tar_read(fr_mortality))` observations.
* Compute surprisals under model, and surprisal probabilities under a GPD with $\alpha=0.01$
* Identify when at least three age groups are anomalous in same year/sex.

## Application to French mortality 

```{r}
tar_read(fig_fr_anomalies)
```

## Application to French mortality 

* 1832, 1849: Cholera outbreaks
* 1870: Franco--Prussian war
* 1871: Repression of the ‘Commune de Paris’
* 1914-1918: World War I
* 1918: Spanish flu outbreak
* 1940: World War II

# Lookout algorithm

## KDE

## Bandwidth selection

## Persistent homology


## Application to Wine quality and prices

# Conclusions

## Conclusions

## Application to test cricket not-outs 
\vspace*{-0.75cm}

```{r}
#| label: fig-notouts0
tar_read(not_outs_plot)
```

\only<2>{\begin{textblock}{8.6}(7,1.7)
\begin{block}{Logistic model}\fontsize{12}{14}\selectfont
$\text{NotOuts} \mid \text{Innings} \sim \text{Binomial}(n=\text{Innings},~ p)$\\
$\log(p / (1- p)) = g(\text{Innings})$
\end{block}
\end{textblock}}

## Application to test cricket not-outs 
\vspace*{-0.75cm}

```{r}
#| label: fig-notouts1
tar_read(not_outs_plot_smooth)
```

\begin{textblock}{8.6}(7,1.7)
\begin{block}{Logistic model}\fontsize{12}{14}\selectfont
$\text{NotOuts} \mid \text{Innings} \sim \text{Binomial}(n=\text{Innings},~ p)$\\
$\log(p / (1- p)) = g(\text{Innings})$
\end{block}
\end{textblock}

## Application to test cricket not-outs 
\vspace*{-0.75cm}

```{r}
#| label: fig-notouts2
#| fig.cap: "Proportion of not outs for each batter as a function of the number of innings they played. The blue line and associated 95% confidence interval shows the probability of a batter not being dismissed as a function of the number of innings they have played."
#| message: false
#| code-fold: false
tar_read(not_outs_plot_jma)
```

\begin{textblock}{8.6}(7,1.7)
\begin{block}{Logistic model}\fontsize{12}{14}\selectfont
$\text{NotOuts} \mid \text{Innings} \sim \text{Binomial}(n=\text{Innings},~ p)$\\
$\log(p / (1- p)) = g(\text{Innings})$
\end{block}
\end{textblock}
