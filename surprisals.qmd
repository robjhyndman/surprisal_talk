---
title: Anomaly detection using&nbsp;surprisals 
author: Rob J Hyndman
date: 28 October 2025
abstract: "I will discuss a probabilistic approach to anomaly detection based on extreme 'surprisal values' aka log scores, equal to minus the log density at each observation. The surprisal approach can be used for any collection of data objects, provided a probability density can be defined on the sample space. It can distinguish anomalies from legitimate observations in a heavy tail, and will identify anomalies that are undetected using methods based on distance measures. I will demonstrate the idea in various real data examples including univariate, multivariate and regression contexts, and when exploring more complicated data objects. I will also briefly outline the underlying theory when the density is known, and when it is estimated using a kernel density estimate. In the latter case, an innovative bandwidth selection method is used based on persistent homology."
toc: true
format:
  presentation-beamer:
    knitr:
      opts_chunk:
        dev: "CairoPDF"
    fig-width: 8
    fig-height: 4.25
    titlegraphic: bg-13.png
    titlecolor: white
    keep-tex: true
include-in-header: header.tex
highlight-style: tango
execute:
  echo: false
  message: false
  warning: false
  cache: false
---

```{r}
#| label: load-packages
library(targets)
library(ggplot2)
tar_source("R/functions.R")
set_ggplot_options()
```


## Coauthors

\placefig{0.2}{1.4}{height=5cm}{figs/sevvandi.png}
\begin{textblock}{5}(0.2,6.7)\centering
  Sevvandi\\ 
  Kandanaarachchi\\
  \emph{CSIRO}
\end{textblock}
\placefig{5.5}{1.4}{height=5cm}{figs/kate.png}
\begin{textblock}{5}(5.5,6.7)\centering
 Kate\\ 
 Turner\\
 \emph{ANU}
\end{textblock}
\placefig{10.8}{1.4}{height=5cm}{figs/david.png}
\begin{textblock}{5}(10.8,6.7)\centering
  David\\ 
  Frazier \\
  \emph{Monash U}
\end{textblock}

## Outline
\vspace*{0.7cm}
\tableofcontents[hideallsubsections]

# Anomalies

## Definitions of anomalies

\begin{block}{} 
\emph{an observation (or a subset of observations) which appears to be inconsistent with the remainder of that set of data.}\\\mbox{}\hfill (Barnett \& Lewis, 1978) 
\end{block}\pause

\begin{block}{}
\emph{an observation which deviates so much from other observations as to arouse suspicion it was generated by a different mechanism.}\hfill (Hawkins, 1980)
\end{block}

## Is this an anomaly?

```{r}
#| fig-height: 1
tar_read(figchisq1)
```

\pause

```{r}
#| fig-height: 1
tar_read(figchisq2)
```

\pause

```{r}
#| fig-height: 1
tar_read(figchisq3)
```

\pause

All points randomly generated from a $\chi^2_5$ distribution.

## Are there any anomalies?

```{r}
tar_read(fig_old_faithful) 
```

## Definitions of anomalies
\fontsize{14}{16}\selectfont
\begin{alertblock}{Definition: Anomaly}
  Given a set of observations $\{y_{1},\dots,y_n\}$ drawn from probability distribution $F$, the \textbf{anomaly score} of $y_i$ is
  $$p_i = \Pr(f(Y) < f(y_i))$$
  where $Y\sim F$ and $f$ is the generalized density of $F$. An observation is an \textbf{anomaly} if $p_i<\alpha$ for some threshold $\alpha > 0$.
\end{alertblock}\pause

* $y_i$ can be a scalar, vector or a more complex object
* $f$ can be a conditional density, and can be known or estimated

## Definitions of anomalies

```{r}
tar_read(figchisq5)
```

## Definitions of anomalies

```{r}
tar_read(fig61)
```

## Anomaly detection: Normal distribution
\vspace*{-0.2cm}

If $F\sim N(\mu,\sigma^2)$, then $p_i = 2\left[1 - \Phi(|y_i - \mu|/\sigma)\right]$\newline
Equivalent to a two-sided p-value from a z-score test.

```{r}
#| fig-height: 3.5
tar_read(fig_normal)
```

## Anomaly detection: Highest density regions
\vspace*{-0.2cm}

HDR with probability $1-\alpha$ is $R_\alpha = \{y: f(y) \ge c_\alpha\}$ where $c_\alpha$ is largest constant s.t. $\Pr(Y \in R_\alpha) \ge 1-\alpha$.\newline
An observation is an anomaly if $y_i \notin R_\alpha$.

```{r}
#| fig-height: 3.1
tar_read(fig_hdr)
```

# Surprisals

## Surprisals

\begin{alertblock}{Definition: Surprisal}
  The \textbf{surprisal} of an observation $y_i$ drawn from probability distribution $F$ with generalized density $f$ is defined as
  $$s_i = -\log f(y_i)$$
\end{alertblock}

* Better known as "log scores" in statistics.
* "Surprisal" coined by Tribus (1961).
* Average surprisal = entropy of random variable
* Sum of surprisals = negative log likelihood

## Anomaly detection using surprisals

Let $G(s) = \Pr(S \le s)$ be the **surprisal distribution** where $S = -\log f(Y)$ and $Y \sim F$. 
$$G(s) = \Pr(-\log f(Y) \le s) = \Pr(f(Y) \ge e^{-s})$$
The **surprisal score** is
$$p_i = 1-G(s_i)$$
and an observation is an **anomaly** if $p_i < \alpha$.

# Extreme value theory and surprisals

## Fisher-Tippett-Gnedenko theorem
\fontsize{13}{14}\selectfont
Consider $n$ iid rvs $S_1, \dots, S_n$ with cdf $G$ and $M_n = \max \{S_1, \dots, S_n\}$. If there exist sequences of constants $\{a_n>0\}$ and $\{b_n\}$ such that
$$\Pr\left\{ (M_n - b_n)/a_n \leq z \right\} \rightarrow H(z) \quad \text{as} \quad n \to \infty,$$
for a non-degenerate cdf $H$, then 
$$H(z) = \exp\left\{ -\left[ 1 + \xi\left(\frac{z - \mu}{\sigma} \right)\right]^{-1/\xi} \right\}$$

* $\xi>0$: Fréchet distribution ($G$ heavy-tailed)
* $\xi\rightarrow0$: Gumbel distribution ($G$ light-tailed)
* $\xi<0$: Weibull distribution ($G$ bounded upper tail)

## Pickands-Balkema-De Haan theorem

If $G$ satisfies the FTG theorem, then the upper tail of $G$ can be approximated by the Generalized Pareto Distribution (GPD): 
$$\Pr(S \le u+s \mid S > u) = 1 - \left( 1 + \frac{\xi s}{\sigma_u} \right)^{-1/\xi}$$
for large enough $u$, where $\sigma_u = \sigma + \xi(u- \mu)$.\pause\vspace*{0.6cm}

### 
\vspace*{-0.3cm}

So in practice, we can approximate the upper tail of many distributions by a GPD.

## Surprisals and EVT

* Suppose we have $n$ iid observations $Y_1,\dots,Y_n$ from distribution $F$ with density $f$.
* Let $S_i = -\log f(Y_i)$ be the surprisal of $Y_i$.
* Then $S_1,\dots,S_n$ are iid from the surprisal distribution $G(s) = \Pr(S \le s)$.
* If $G$ satisfies the FTG theorem, then we can approximate the upper tail of $G$ by a GPD.

## Three-type theorem for surprisals

**A1: Sub-Gaussian**: $S =-\log f(Y)$ satisfies, for all $\lambda\in\mathbb{R}$, and some $\nu>0$,
$\E \exp\{\lambda (S-\E[S])\}\le \exp\{\lambda^2 \nu^2/2\}$.`\\`{=latex}
**A2: Sub-exponential**: $S$ is sub-exponential with parameters $\nu$ and $b$, i.e.,
$\E \exp\{\lambda(S-\E[S])\} \leq \exp\{\lambda^2\nu^2/ 2\}$ for all $|\lambda|< 1/b$.`\\`{=latex}
**A3: Polynomial**: $|S|$ has polynomial moments of order $p\ge 1$; i.e., $\mathbb{E}[|S|^p] \le C^p$ for some $C>0$ such that $C^p-1>0$.

* A1 satisfied when $f$ has bounded support
* A2 satisfied when log $f$ unbounded below, and light tails (e.g., Gaussian)
* A3 satisfied when $f$ has heavy tails (e.g., t with df $\ge 3$)

## Three-type theorem for surprisals
\fontsize{13}{14}\selectfont
Let $y_1,\dots,y_n$ be an iid sequence from $F$, $S =-\log f(Y)$ where $Y\sim F$, $s_i = -\log f(y_i)$, and $M_n=\max\{s_1,\dots,s_n\}$.

1. Under A1:
$$
  \sup_{s:s>0}\left|\Pr\left\{\big|M_n-\E[S]\big|\ge \sqrt{2\nu^2 s}+\sqrt{2\nu^2\log(2n)}\right\}-e^{-s}\right|=o(1).
$$
2. Under A2:
$$
  \sup_{s:s>1/b}\left|\Pr\Big\{\big|M_n-\E[S]\big|\ge (2b)s+(2b)\log(2n)\Big\}-e^{-e^{-s}}\right|=o(1).
$$
3. Under A3:
$$
	\sup_{s:s>C}\left|\Pr\Big\{\big|M_n-\E[S]\big|\ge (Csn^{1/p})\Big\}-e^{-s^{-p}}\right|=o(1).
$$


## Three-type theorem for surprisals

* If surprisal has Gaussian like tails, then maximum surprisal is a reversed Weibull;
* If surprisal only has an exponential moment, then maximum surprisal is Gumbel; 
* If surprisal only has a polynomial moment, then maximum surprisal is Fréchet. 

\vspace*{0.2cm}
 
### Corollary
\vspace*{-0.3cm}

upper tail of the surprisal distribution can be approximated by a GPD, even if the assumed distribution $F$ is incorrect, provided one of A1--A3 is satisfied.

## Application to wine quality and prices

```{r}
tar_read(fig_wine)
```

## Application to wine quality and prices
\vspace*{-0.3cm}
Proposed model: $\log \text{Price} \mid \text{Points} \sim N(a + b\text{Points}, \sigma^2)$.

```{r}
#| fig-height: 3.8
tar_read(wine_model_plot)
```

## Application to wine quality and prices
\vspace*{-0.3cm}
Proposed model: $\log \text{Price} \mid \text{Points} \sim N(a + b\text{Points}, \sigma^2)$.

```{r}
#| fig-height: 3.8
tar_read(wine_loo_surprisals)
```

## Application to wine quality and prices
\vspace*{-0.3cm}
Proposed model: $\log \text{Price} \mid \text{Points} \sim N(a + b\text{Points}, \sigma^2)$.

```{r}
#| fig-height: 3.8
tar_read(wine_loo_anomalies)
```

\begin{textblock}{4.9}(1.9,2.0)
\begin{block}{}\footnotesize
GPD fitted to top 10\% of surprisals computed from LOO predictive densities.
\end{block}
\end{textblock}


\begin{textblock}{1.85}(13.9,0.95)
\begin{block}{}\small
\textcolor{red}{$\alpha = 0.001$}
\end{block}
\end{textblock}

## Application to wine quality and prices

```{r}
tar_read(fig_wine2)
```

\begin{textblock}{1.85}(13.9,0.95)
\begin{block}{}\small
\textcolor{red}{$\alpha = 0.001$}
\end{block}
\end{textblock}


## Application to wine quality and prices
\vspace*{-0.2cm}
\structure{Anomalies detected ($\alpha=0.001$):}
\fontsize{12}{14}\selectfont\vspace*{-0.3cm}

```{r}
tar_read(wine_anomalies) |>
  kableExtra::kbl(booktabs = TRUE, linesep="")
```

## French mortality

```{r}
tar_read(fig_fr_mortality)
```

## French mortality

```{r}
tar_read(fig_fr_mortality2)
```

## Application to French mortality 
\fontsize{14}{16}\selectfont

Model: $\log y_t \sim N(m_t, a_t)$ where $m_t$ and $a_t$ are locally and robustly estimated in a window of size $2h+1$ around time $t$:
\begin{align*}
\hat{m}_t &= \text{median}(\log y_{t-h},\dots,\log y_{t+h}) \\
\hat{a}_t &= 1.4826 \times \text{median}(|\log y_{t-h}-\hat{m}_t|,\dots,|\log y_{t+h}-\hat{m}_t|)
\end{align*}\pause\vspace*{-0.8cm}

* Male and female data from 1816--1999, over ages 0--85: `r NROW(tar_read(fr_mortality))` observations.
* Compute surprisals under model, and surprisal probabilities under a GPD with $\alpha=0.01$
* Identify when at least three age groups are anomalous in same year/sex.

## Application to French mortality 

```{r}
tar_read(fig_fr_anomalies)
```

## Application to French mortality 

* 1832, 1849: Cholera outbreaks
* 1870: Franco--Prussian war
* 1871: Repression of the ‘Commune de Paris’
* 1914--1918: World War I
* 1918: Spanish flu outbreak
* 1940: World War II

# Lookout algorithm

## Lookout algorithm

* Compute Kernel Density Estimate (KDE) of the observations, with a bandwidth matrix chosen using persistent homology.
* Compute Leave-One-Out (LOO) KDE values
* Compute surprisals from LOO KDE values
* Fit a Generalized Pareto Distribution (GPD) to the largest surprisals
* Compute surprisal probabilities $p_i$ from the fitted GPD
* An observation is an anomaly if $p_i < \alpha$.

## Kernel density estimation
\fontsize{14}{15}\sf\vspace*{-0.3cm}

Observations: $\bm{y}_{i} \in \mathbb{R}^m$ for $i \in \{1, \dots, n\}$. 

\begin{block}{KDE}
\centerline{$\displaystyle\hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n |\bm{H}|^{-1/2} K\big(\bm{H}^{-1/2}(\bm{y} - \bm{y}_i)\big),$}
\end{block}\vspace*{-0.25cm}

* $K$ is a square-integrable spherically-symmetric function, bounded below by 0, with a finite second-order moment and unit integral.
* $\bm{H}$ is a symmetric $m\times m$ positive-definite matrix.

\pause\vspace*{0.1cm}

### LOO KDE values

\centerline{$\displaystyle f_{-i} = \frac{1}{n-1} \big( n \hat{f}(\bm{y}_i) - \bm{H}^{-1/2} K(\bm{0}) \big)$}

## Kernel density estimation 
\vspace*{-0.3cm}

\begin{block}{}
\centerline{$\displaystyle\hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n |\bm{H}|^{-1/2} K\big(\bm{H}^{-1/2}(\bm{y} - \bm{y}_i)\big),$}
\end{block}

\centerline{\animategraphics[controls,buttonsize=0.3cm,width=12.5cm]
    {6}{"figs/kde_"}{1}{99}}

## Kernel density estimation (consistency)

Let $Y_1,\dots,Y_n$ be iid from square integrable density function $f$. Then KDE is a *consistent* estimator of $f$ if
  $$
  \lim_{n\rightarrow0} \text{E}\left[\left(\hat{f}(\bm{u}) - f(\bm{u})\right)^2 du\right] = 0
  $$

###
For $\hat{f}$ to be a consistent estimator, $\bm{H}$ must be a symmetric positive-definite matrix that satisfies the following conditions:

  * All elements of $\bm{H}$ approach zero as $n\rightarrow\infty$
  * $n^{-1}|\bm{H}|^{-1/2} \rightarrow 0$ as $n\rightarrow\infty$

## Persistent homology

\centerline{\animategraphics[controls,buttonsize=0.3cm,width=12.5cm]
    {6}{"figs/rips_"}{1}{99}}

## Persistent homology

```{r}
tar_read(barcode)
```
## Lookout algorithm
\fontsize{13}{15}\sf\vspace*{-0.2cm}

1. $\bm{Y}=$ data matrix with rows $\bm{y}_1, \dots, \bm{y}_n$. 
2. $\hat{\bm{\Sigma}}=$ orthogonalized Gnanadesikan-Kettenring estimate of $\text{Cov}(\bm{Y})$, with eigendecomposition $\hat{\bm{\Sigma}} = \bm{U}\bm{D}\bm{U}^\top$.
3. Rotate and scale the data: $\bm{Z} = \bm{U}\bm{Y}$.
4. Compute persistence homology barcode of $\bm{Z}$ for dim zero using Vietoris-Rips diameter; obtain ordered death diameters $\{d_i\}_{i = 1}^n$.
5. $d^* =$ $\gamma$ sample quantile computed from $\{ d_i \}_{i=1}^n$.
6. Compute kde: $f_i = \hat{f}(\bm{z}_i)$, $i=1,\dots,n$, where $\bm{H} = (d^*)^{2/m}\bm{I}_m$.
7. Compute LOO kde values $f_{-i} = \frac{1}{n-1} \big( n f_i - \bm{H}^{-1/2} K(\bm{0}) \big)$, $i=1,\dots,n$.
8. Fit GPD to largest $1-\beta$ of surprisals $\{-\log f_i\}_{i=1}^n$, constraining shape parameter to be non-positive.
9. $p_i = (1-\beta)P(-\log f_{-i} \mid \hat{\mu}, \hat{\sigma}, \hat{\xi})$, $P$ = GPD cdf.

## Old Faithful eruptions

```{r}
tar_read(fig_old_faithful)
```

## Old Faithful eruptions

```{r}
tar_read(fig_old_faithful2)
```

## Old Faithful eruptions

```{r}
tar_read(fig_old_faithful3)
```

## Old Faithful eruptions

```{r}
tar_read(fig_old_faithful4)
```

\begin{textblock}{1.4}(2.2,1.75)
\begin{block}{}\footnotesize\color{red}
$\alpha = 0.01$
\end{block}
\end{textblock}

## Old Faithful eruptions

```{r}
tar_read(fig_old_faithful5)
```

\begin{textblock}{1.4}(2.2,1.75)
\begin{block}{}\footnotesize\color{red}
$\alpha = 0.01$
\end{block}
\end{textblock}

## Old Faithful eruptions
\fontsize{12}{15}\sf\vspace*{-0.3cm}\tabcolsep=0.1cm

```{r}
tar_read(of_lookout_prob) |>
  kableExtra::kbl(booktabs = TRUE, linesep="")
```

# Conclusions

## Conclusions

## Application to test cricket not-outs 
\vspace*{-0.75cm}

```{r}
#| label: fig-notouts0
tar_read(not_outs_plot)
```

\only<2>{\notoutmodel}

## Application to test cricket not-outs 
\vspace*{-0.75cm}

```{r}
#| label: fig-notouts1
tar_read(not_outs_plot_smooth)
```
\notoutmodel

## Application to test cricket not-outs 
\vspace*{-0.75cm}

```{r}
#| label: fig-notouts2
#| fig.cap: "Proportion of not outs for each batter as a function of the number of innings they played. The blue line and associated 95% confidence interval shows the probability of a batter not being dismissed as a function of the number of innings they have played."
#| message: false
#| code-fold: false
tar_read(not_outs_plot_jma)
```

\notoutmodel